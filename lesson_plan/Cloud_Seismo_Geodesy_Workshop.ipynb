{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4fb2ac",
   "metadata": {},
   "source": [
    "\n",
    "# **Workshop Notebook: Preparing Cloud-Based Seismic & Geodetic Data for Research**  \n",
    "\n",
    "**Duration:** ~4 hours  \n",
    "\n",
    "**Audience:** Early-career professionals in geophysics, seismology, geodesy, and Earth data science.  \n",
    "\n",
    "This notebook follows the workshop structure with one hands-on Python exercise per part.  \n",
    "If you do not have access to cloud buckets in your environment, each exercise includes a **synthetic fallback** so you can complete it offline.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Setup\n",
    "\n",
    "> You can run with your existing environment, or install recommended packages (comment/uncomment as needed).  \n",
    "> If running in a managed JupyterLab (e.g., institutional hub), consult local docs.\n",
    "\n",
    "```bash\n",
    "# (optional) in a terminal:\n",
    "# mamba create -n cloud-geo python=3.11 -y\n",
    "# mamba activate cloud-geo\n",
    "# mamba install -c conda-forge obspy numpy scipy pandas matplotlib scikit-learn pyproj h5py dask fsspec s3fs -y\n",
    "# pip install pygmt==0.12.0  # optional for mapping\n",
    "```\n",
    "\n",
    "**Data options:**  \n",
    "- **Option A (recommended):** Point to your cloud buckets (e.g., `s3://...`, `gs://...`) or local files.  \n",
    "- **Option B:** Use the **synthetic generators** included below to complete the exercises with placeholder data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports used throughout the workshop\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import pathlib\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional imports guarded by try/except (so notebook still runs if missing)\n",
    "try:\n",
    "    from obspy import read, Stream, Trace, UTCDateTime\n",
    "except Exception as e:\n",
    "    read = None\n",
    "    Stream = None\n",
    "    Trace = None\n",
    "    UTCDateTime = None\n",
    "    print(\"ObsPy not available. Synthetic seismic fallback will be used.\")\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "except Exception as e:\n",
    "    RandomForestClassifier = None\n",
    "    print(\"scikit-learn not available; ML examples will be illustrative only.\")\n",
    "\n",
    "try:\n",
    "    import fsspec  # to read cloud data like s3:// or gs://\n",
    "except Exception:\n",
    "    fsspec = None\n",
    "    print(\"fsspec not available; cloud file access will be skipped.\")\n",
    "\n",
    "# Plotting config\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 4)\n",
    "plt.rcParams[\"axes.grid\"] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07241de9",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1 â€” Introduction & Context (Exercise ~30â€“40 min)\n",
    "\n",
    "**Learning goals**  \n",
    "- Recognize key file formats (MiniSEED, RINEX) and metadata.  \n",
    "- Explore small seismic & GNSS time series and visualize them.  \n",
    "- Understand how cloud URIs are accessed in Python with `fsspec` (if available).\n",
    "\n",
    "### Exercise 1 â€” Quick Data Tour & Visualization\n",
    "**Task:** Load a short seismic waveform and a short GNSS time series from either a cloud bucket or local files; if unavailable, generate **synthetic** data. Plot both.\n",
    "\n",
    "**What youâ€™ll submit:** One or two plots + 2â€“3 bullet points describing anything notable (e.g., noise level, sampling rate, gaps).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909131e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Configuration: set paths if you have real data ====\n",
    "SEISMIC_PATH = os.environ.get(\"SEISMIC_PATH\", \"\")  # e.g., \"s3://my-bucket/demos/event.mseed\" or \"./data/event.mseed\"\n",
    "GNSS_PATH    = os.environ.get(\"GNSS_PATH\", \"\")     # e.g., \"s3://my-bucket/gnss/sample_rinex.obs\" or \"./data/sample_rinex.csv\"\n",
    "\n",
    "# ==== Helpers ====\n",
    "def read_text_with_fsspec(path):\n",
    "    if fsspec is None:\n",
    "        raise RuntimeError(\"fsspec not installed; cannot read cloud paths.\")\n",
    "    with fsspec.open(path, 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def maybe_read_mseed(path):\n",
    "    if not path or read is None:\n",
    "        return None\n",
    "    try:\n",
    "        if fsspec and (path.startswith(\"s3://\") or path.startswith(\"gs://\")):\n",
    "            with fsspec.open(path, 'rb') as f:\n",
    "                return read(f)\n",
    "        else:\n",
    "            return read(path)\n",
    "    except Exception as e:\n",
    "        print(\"Could not read MiniSEED:\", e)\n",
    "        return None\n",
    "\n",
    "def synthetic_seismic(npts=4000, dt=0.005, f0=5.0, noise=0.2):\n",
    "    t = np.arange(npts) * dt\n",
    "    # Ricker wavelet with noise\n",
    "    pi2 = (np.pi**2)\n",
    "    r = (1 - 2*pi2*(f0**2)*(t-2)**2) * np.exp(-pi2*(f0**2)*(t-2)**2)\n",
    "    x = r + noise * np.random.randn(npts)\n",
    "    return t, x\n",
    "\n",
    "def synthetic_gnss_days(ndays=5, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    time_idx = pd.date_range(\"2023-01-01\", periods=ndays*24, freq=\"H\")\n",
    "    # Simulate vertical displacement (mm) with diurnal cycle\n",
    "    trend = np.linspace(0, 5, len(time_idx))\n",
    "    diurnal = 2.0*np.sin(2*np.pi*(np.arange(len(time_idx))%24)/24.0)\n",
    "    noise = rng.normal(0, 0.8, size=len(time_idx))\n",
    "    disp_mm = trend + diurnal + noise\n",
    "    df = pd.DataFrame({\"time\": time_idx, \"disp_mm\": disp_mm}).set_index(\"time\")\n",
    "    return df\n",
    "\n",
    "# ==== Load or synthesize data ====\n",
    "st = maybe_read_mseed(SEISMIC_PATH)\n",
    "if st is None:\n",
    "    print(\"Using synthetic seismic trace.\")\n",
    "    t, x = synthetic_seismic()\n",
    "    seismic_df = pd.DataFrame({\"t_s\": t, \"amp\": x})\n",
    "else:\n",
    "    tr = st[0]\n",
    "    t = np.arange(tr.stats.npts) * tr.stats.delta\n",
    "    seismic_df = pd.DataFrame({\"t_s\": t, \"amp\": tr.data})\n",
    "\n",
    "# GNSS\n",
    "if GNSS_PATH and os.path.exists(GNSS_PATH):\n",
    "    gnss_df = pd.read_csv(GNSS_PATH, parse_dates=[\"time\"]).set_index(\"time\")\n",
    "else:\n",
    "    print(\"Using synthetic GNSS time series.\")\n",
    "    gnss_df = synthetic_gnss_days()\n",
    "\n",
    "# ==== Plot ====\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(seismic_df[\"t_s\"], seismic_df[\"amp\"])\n",
    "ax.set_title(\"Seismic waveform (synthetic if no MiniSEED provided)\")\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "gnss_df[\"disp_mm\"].plot(ax=ax)\n",
    "ax.set_title(\"GNSS vertical displacement (mm) â€” synthetic if no file provided\")\n",
    "ax.set_ylabel(\"Displacement (mm)\")\n",
    "plt.show()\n",
    "\n",
    "# ==== Your notes ====\n",
    "print(\"ğŸ“ TODO: In a markdown cell, write 2â€“3 bullet points about what you observe.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f659772",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2 â€” Data Access & Preparation (Exercise ~45â€“60 min)\n",
    "\n",
    "**Learning goals**  \n",
    "- Ingest seismic MiniSEED and GNSS time series from cloud or local storage.  \n",
    "- Apply **preprocessing**: detrending, filtering, resampling, outlier removal.  \n",
    "- Persist cleaned outputs back to cloud/local (parquet/csv).\n",
    "\n",
    "### Exercise 2 â€” Build a Reusable Prep Function\n",
    "**Task:** Implement the TODOs to complete `prep_seismic()` and `prep_gnss()` functions, then run them on your data (or synthetic). Save results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98925fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Seismic preprocessing ====\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def butter_bandpass(low, high, fs, order=4):\n",
    "    ny = 0.5 * fs\n",
    "    b, a = butter(order, [low/ny, high/ny], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def prep_seismic(df, fs=200.0, band=(1.0, 20.0)):\n",
    "    # Detrend and bandpass filter a seismic time series DataFrame (columns: t_s, amp)\n",
    "    df = df.copy()\n",
    "    df[\"amp_dt\"] = df[\"amp\"] - df[\"amp\"].mean()\n",
    "\n",
    "    # Bandpass filter\n",
    "    b, a = butter_bandpass(band[0], band[1], fs)\n",
    "    try:\n",
    "        df[\"amp_filt\"] = filtfilt(b, a, df[\"amp_dt\"])\n",
    "    except Exception as e:\n",
    "        print(\"Filter failed (try adjusting band or fs). Using detrended only.\", e)\n",
    "        df[\"amp_filt\"] = df[\"amp_dt\"]\n",
    "    return df[[\"t_s\", \"amp_filt\"]]\n",
    "\n",
    "# ==== GNSS preprocessing ====\n",
    "def hampel_filter(series, k=5, t0=3.0):\n",
    "    # Simple Hampel outlier filter.\n",
    "    n = len(series)\n",
    "    new_series = series.copy().astype(float)\n",
    "    L = 1.4826\n",
    "    for i in range(n):\n",
    "        i1 = max(i-k, 0)\n",
    "        i2 = min(i+k, n-1)\n",
    "        window = series.iloc[i1:i2+1]\n",
    "        median = window.median()\n",
    "        mad = L * np.median(np.abs(window - median))\n",
    "        if mad == 0:\n",
    "            continue\n",
    "        if np.abs(series.iloc[i] - median) > t0 * mad:\n",
    "            new_series.iloc[i] = median\n",
    "    return new_series\n",
    "\n",
    "def prep_gnss(df):\n",
    "    # Clean GNSS displacement: Hampel outliers, linear detrend, hourly resample.\n",
    "    y = df[\"disp_mm\"]\n",
    "    y_clean = hampel_filter(y, k=6, t0=3.5)\n",
    "    x = np.arange(len(y_clean))\n",
    "    coeffs = np.polyfit(x, y_clean.values, 1)\n",
    "    trend = np.polyval(coeffs, x)\n",
    "    detrended = y_clean.values - trend\n",
    "    out = pd.DataFrame({\"disp_mm_clean\": detrended}, index=df.index)\n",
    "    out = out.resample(\"1H\").mean().interpolate()\n",
    "    return out\n",
    "\n",
    "# ==== Apply to our data ====\n",
    "# Seismic fs estimation if synthetic:\n",
    "if len(seismic_df) > 1:\n",
    "    dt = np.mean(np.diff(seismic_df[\"t_s\"]))\n",
    "    fs = 1.0 / dt\n",
    "else:\n",
    "    fs = 200.0\n",
    "\n",
    "seis_clean = prep_seismic(seismic_df, fs=fs, band=(1.0, 20.0))\n",
    "gnss_clean = prep_gnss(gnss_df)\n",
    "\n",
    "# ==== Save to disk (you can change to cloud URIs if fsspec available) ====\n",
    "outdir = pathlib.Path(\"./prep_outputs\")\n",
    "outdir.mkdir(exist_ok=True, parents=True)\n",
    "seis_clean.to_parquet(outdir / \"seismic_clean.parquet\")\n",
    "gnss_clean.to_parquet(outdir / \"gnss_clean.parquet\")\n",
    "\n",
    "print(\"Saved cleaned outputs to:\", outdir.resolve())\n",
    "\n",
    "# ==== Visualize results ====\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(seis_clean[\"t_s\"], seis_clean[\"amp_filt\"])\n",
    "ax.set_title(\"Seismic (cleaned)\")\n",
    "ax.set_xlabel(\"Time (s)\"); ax.set_ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "gnss_clean[\"disp_mm_clean\"].plot(ax=ax)\n",
    "ax.set_title(\"GNSS (cleaned, detrended)\")\n",
    "ax.set_ylabel(\"Displacement (mm, detrended)\")\n",
    "plt.show()\n",
    "\n",
    "# ğŸ“ TODO: Note any parameter choices you changed and why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfcf5fc",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3 â€” Cloud-Based Analysis & ML Integration (Exercise ~60â€“75 min)\n",
    "\n",
    "**Learning goals**  \n",
    "- Build a minimal ML pipeline for **seismic event detection** (binary classification) with synthetic/semi-real features.  \n",
    "- (Optional) Estimate an atmospheric proxy (e.g., PWV surrogate) from GNSS using simple regression as a warm-up.\n",
    "\n",
    "### Exercise 3A â€” Seismic Event Classifier (Random Forest)\n",
    "**Task:** Create features from short seismic windows and train a classifier to distinguish **event** vs **noise**.  \n",
    "If you lack labeled data, we will **synthesize** events (Ricker wavelets) + noise.\n",
    "\n",
    "**Deliverables:**  \n",
    "- Confusion matrix + classification report  \n",
    "- A brief note: which features mattered and how you would improve this for real data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d455588",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Feature engineering for seismic snippets ====\n",
    "def window_features(x, fs):\n",
    "    # Extract simple features from a 1D window x.\n",
    "    feats = {}\n",
    "    feats[\"rms\"] = np.sqrt(np.mean(x**2))\n",
    "    feats[\"peak\"] = np.max(np.abs(x))\n",
    "    feats[\"zcr\"] = np.mean(np.abs(np.diff(np.sign(x))))  # zero crossing rate (approx)\n",
    "    # spectral centroid proxy\n",
    "    X = np.fft.rfft(x)\n",
    "    freqs = np.fft.rfftfreq(len(x), d=1.0/fs)\n",
    "    ps = np.abs(X)**2\n",
    "    if ps.sum() > 0:\n",
    "        feats[\"spec_centroid\"] = (freqs * ps).sum() / ps.sum()\n",
    "    else:\n",
    "        feats[\"spec_centroid\"] = 0.0\n",
    "    return feats\n",
    "\n",
    "def make_dataset_from_series(seis, fs, win_s=2.0, step_s=1.0, p_event=0.3, seed=0):\n",
    "    # Synthesize labels: inject Ricker-like 'events' in random windows for demo purposes.\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = seis[\"amp_filt\"].values\n",
    "    n = len(x)\n",
    "    win = int(win_s * fs)\n",
    "    step = int(step_s * fs)\n",
    "    X, y = [], []\n",
    "    i = 0\n",
    "    while i + win <= n:\n",
    "        seg = x[i:i+win].copy()\n",
    "        # Randomly inject an 'event'\n",
    "        is_event = rng.random() < p_event\n",
    "        if is_event:\n",
    "            t = np.arange(win)/fs\n",
    "            f0 = rng.uniform(3.0, 8.0)\n",
    "            r = (1 - 2*(np.pi**2)*(f0**2)*(t-0.5)**2)*np.exp(-(np.pi**2)*(f0**2)*(t-0.5)**2)\n",
    "            seg = seg + r * rng.uniform(1.5, 3.0)\n",
    "        feats = window_features(seg, fs)\n",
    "        X.append([feats[k] for k in [\"rms\",\"peak\",\"zcr\",\"spec_centroid\"]])\n",
    "        y.append(1 if is_event else 0)\n",
    "        i += step\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Build dataset\n",
    "X, y = make_dataset_from_series(seis_clean, fs=fs, win_s=2.0, step_s=1.0, p_event=0.35, seed=42)\n",
    "print(\"Features shape:\", X.shape, \"Labels:\", y.shape, \"Positives:\", y.sum())\n",
    "\n",
    "if RandomForestClassifier is None:\n",
    "    print(\"scikit-learn not available; skipping training.\")\n",
    "else:\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, random_state=0, stratify=y)\n",
    "    clf = RandomForestClassifier(n_estimators=150, random_state=0)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    y_pred = clf.predict(X_te)\n",
    "\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y_te, y_pred))\n",
    "\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(y_te, y_pred))\n",
    "\n",
    "    # Simple feature importance\n",
    "    importances = clf.feature_importances_\n",
    "    feat_names = [\"rms\",\"peak\",\"zcr\",\"spec_centroid\"]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(range(len(importances)), importances)\n",
    "    ax.set_xticks(range(len(importances)))\n",
    "    ax.set_xticklabels(feat_names, rotation=0)\n",
    "    ax.set_title(\"Feature importance (RandomForest)\")\n",
    "    plt.show()\n",
    "\n",
    "# ğŸ“ TODO: Note how you would obtain *real* labels (e.g., picks/catalog) and extend features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472c62f",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 3B (Optional) â€” GNSS Atmospheric Proxy (Simple Regression)\n",
    "\n",
    "**Task:** Using the cleaned GNSS time series, create a **toy surrogate** for PWV (e.g., linear combo of detrended displacement and diurnal index) and fit a regression model.  \n",
    "This is illustrative; in production you would use proper GNSS meteorology products.\n",
    "\n",
    "**Deliverables:**  \n",
    "- Train/test score  \n",
    "- A plot of predicted vs. target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167af471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a toy target (PWV surrogate) from GNSS trends + diurnal component\n",
    "rng = np.random.default_rng(0)\n",
    "idx = gnss_clean.index\n",
    "h = idx.hour.values\n",
    "diurnal = np.sin(2*np.pi*h/24.0)\n",
    "noise = rng.normal(0, 0.3, size=len(idx))\n",
    "target = 0.7*gnss_clean[\"disp_mm_clean\"].values + 0.8*diurnal + noise\n",
    "\n",
    "X = np.c_[gnss_clean[\"disp_mm_clean\"].values, diurnal]\n",
    "y = target\n",
    "\n",
    "if RandomForestClassifier is None:\n",
    "    print(\"scikit-learn not available; showing scatter only.\")\n",
    "    plt.scatter(X[:,0], y, s=10)\n",
    "    plt.title(\"PWV surrogate vs GNSS detrended displacement\")\n",
    "    plt.xlabel(\"disp_mm_clean\"); plt.ylabel(\"PWV surrogate\")\n",
    "    plt.show()\n",
    "else:\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "    regr = RandomForestRegressor(n_estimators=200, random_state=0)\n",
    "    regr.fit(X_tr, y_tr)\n",
    "    y_hat = regr.predict(X_te)\n",
    "    print(\"R^2:\", r2_score(y_te, y_hat))\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_te, y_hat)))\n",
    "\n",
    "    # Plot predicted vs target\n",
    "    plt.scatter(y_te, y_hat, s=12)\n",
    "    plt.xlabel(\"Target (surrogate)\"); plt.ylabel(\"Prediction\")\n",
    "    plt.title(\"GNSS PWV surrogate â€” predicted vs. target\")\n",
    "    plt.plot([y_te.min(), y_te.max()], [y_te.min(), y_te.max()])\n",
    "    plt.show()\n",
    "\n",
    "# ğŸ“ TODO: List the *real* inputs you would use for PWV (e.g., ZTD + met data) and references/tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7bb905",
   "metadata": {},
   "source": [
    "\n",
    "## Part 4 â€” Reproducibility, Collaboration & Next Steps (Exercise ~30â€“45 min)\n",
    "\n",
    "**Learning goals**  \n",
    "- Capture parameters and environment for reproducibility.  \n",
    "- Save intermediate artifacts and a minimal **pipeline** definition.  \n",
    "- Discuss FAIR/ethics/licensing considerations.\n",
    "\n",
    "### Exercise 4 â€” Make It Reproducible\n",
    "**Task:**  \n",
    "1) Save your key parameters to a JSON **run config**.  \n",
    "2) Package your cleaned outputs and model artifacts into a versioned folder.  \n",
    "3) (Optional) Write a minimal `environment.yml` or `requirements.txt` and a tiny `Makefile` to run the pipeline.\n",
    "\n",
    "**Deliverables:**  \n",
    "- `runs/run_YYYYmmdd_HHMM/config.json`  \n",
    "- Saved cleaned parquet files (already in `prep_outputs/`) and any trained models.  \n",
    "- (Optional) `requirements.txt` and `Makefile` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d18d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, pathlib, shutil\n",
    "from datetime import datetime\n",
    "\n",
    "run_id = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "run_dir = pathlib.Path(\"runs\") / f\"run_{run_id}\"\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    \"seismic\": {\"band\": [1.0, 20.0], \"fs\": float(fs)},\n",
    "    \"gnss\": {\"resample\": \"1H\", \"outlier_filter\": \"Hampel(k=6,t0=3.5)\"},\n",
    "    \"ml\": {\"seis_win_s\": 2.0, \"seis_step_s\": 1.0, \"rf_n_estimators\": 150}\n",
    "}\n",
    "\n",
    "with open(run_dir / \"config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# Copy artifacts if present\n",
    "artifacts = [\"prep_outputs/seismic_clean.parquet\", \"prep_outputs/gnss_clean.parquet\"]\n",
    "for a in artifacts:\n",
    "    p = pathlib.Path(a)\n",
    "    if p.exists():\n",
    "        shutil.copy(p, run_dir / p.name)\n",
    "\n",
    "print(\"Saved run config & artifacts to:\", run_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ba39ba",
   "metadata": {},
   "source": [
    "\n",
    "### Optional: Minimal `requirements.txt`, `Makefile`, & Next Steps\n",
    "\n",
    "You can place these in your repo to standardize runs.\n",
    "\n",
    "**requirements.txt**\n",
    "```\n",
    "obspy\n",
    "numpy\n",
    "scipy\n",
    "pandas\n",
    "matplotlib\n",
    "scikit-learn\n",
    "pyproj\n",
    "fsspec\n",
    "s3fs\n",
    "```\n",
    "\n",
    "**Makefile**\n",
    "```\n",
    "run:\n",
    "\tpython -m jupyter nbconvert --to notebook --execute Cloud_Seismo_Geodesy_Workshop.ipynb --inplace\n",
    "```\n",
    "\n",
    "**Next steps / discussion prompts**\n",
    "- What metadata would you store with each run (sensor IDs, event IDs, GNSS station list)?  \n",
    "- How would you align/merge seismic picks with catalogs to build **real** labels?  \n",
    "- Which cloud services (S3/GS/Azure) will you target, and what IAM/permissions are required?  \n",
    "- How will you implement FAIR principles & licensing in your project?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763633c2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Wrap-up & Reflection\n",
    "\n",
    "**ğŸ“ Prompt:** In 4â€“6 bullet points, summarize your key takeaways and note one improvement you would make to:  \n",
    "- your preprocessing choices  \n",
    "- your ML features/labels  \n",
    "- your reproducibility setup\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}